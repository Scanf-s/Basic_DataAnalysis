{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "data = {\n",
    "    \"message\": \"success\"\n",
    "}\n",
    "\n",
    "serialized = json.dumps(data) # 직렬화\n",
    "print(serialized)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = json.loads(serialized) # 역직렬화\n",
    "print(data)"
   ],
   "id": "cf30f8d1953447",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "book_data: str = \"\"\"\n",
    "[\n",
    "    {\"name\": \"혼자 공부하는 데이터분석\", \"author\": \"박해선\", \"year\": 2022},\n",
    "    {\"name\": \"혼자 공부하는 머신러닝+딥러닝\", \"author\": \"박해선\", \"year\": 2020}\n",
    "]\n",
    "\"\"\"\n",
    "deserialized = json.loads(book_data)\n",
    "print(deserialized[1][\"year\"])"
   ],
   "id": "88195f20ee7a6263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# 25.01.14 기준 read_json이 deprecated 될 예정이라고 뜬다\n",
    "# 해결하기 위해서는 import io -> io.StringIO로 json을 감싸야함\n",
    "pd.read_json(io.StringIO(book_data)) # pandas 데이터프레임으로 json 데이터를 자동으로 역직렬화\n"
   ],
   "id": "d313806252c9e3f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xml_str = \"\"\"\n",
    "<book>\n",
    "    <name>혼자 공부하는 데이터분석</name>\n",
    "    <author>박해선</author>\n",
    "    <year>2022</year>\n",
    "</book>\n",
    "\"\"\"\n",
    "\n",
    "import xml.etree.ElementTree as et\n",
    "book = et.fromstring(xml_str) # x_str xml 문자열을 실제 XML로 변환 (직렬화)\n",
    "print(type(book)) # 타입 조회\n",
    "print(book.tag) # 최상위 Element 조회"
   ],
   "id": "5ddf9494833bdec0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 자식 Element 조회 시\n",
    "book_childs = list(book)\n",
    "print(*book_childs)\n",
    "\n",
    "for child in book_childs:\n",
    "    print(child.tag + \" : \" + child.text)\n"
   ],
   "id": "50b439c56fb8bf66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xml_str = \"\"\"\n",
    "<books>\n",
    "    <book>\n",
    "        <name>혼자 공부하는 데이터분석</name>\n",
    "        <author>박해선</author>\n",
    "        <year>2022</year>\n",
    "    </book>\n",
    "    <book>\n",
    "        <name>혼자 공부하는 머신러닝+딥러닝</name>\n",
    "        <author>박해선</author>\n",
    "        <year>2022</year>\n",
    "    </book>\n",
    "</books>\n",
    "\"\"\"\n",
    "books = et.fromstring(xml_str)\n",
    "print(books.tag) # 최상위 Element"
   ],
   "id": "2767225df1392a52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for book in books.findall('book'): # 여러개의 자식 Element 조회 시\n",
    "    for element in book:\n",
    "        print(element.tag + \" : \" +element.text)"
   ],
   "id": "8eb217c6bf6d6a55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# API 호출을 통해 데이터 수집하기 - 서울시 공공자전거 실시간 대여 현황 API",
   "id": "c118dc979fd7b081"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Requests 라이브러리를 사용해서 API를 호출하는 방법\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "API_KEY = os.getenv(\"PUBLIC_BICYCLE_API_KEY\")\n",
    "if API_KEY is None:\n",
    "    print(\"PUBLIC_BICYCLE_API_KEY is not defined\")\n",
    "    exit(1)\n",
    "\n",
    "start_offset=1\n",
    "limit=5"
   ],
   "id": "f5395f9cbe3f7180",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.get(\n",
    "    url=f\"http://openapi.seoul.go.kr:8088/{API_KEY}/json/bikeList/{start_offset}/{limit}/\",\n",
    ")\n",
    "public_bike_dataframe = None\n",
    "if response.status_code == 200:\n",
    "    fetched_data = response.json()\n",
    "\n",
    "    rows = fetched_data[\"rentBikeStatus\"][\"row\"]\n",
    "\n",
    "    public_bike_dataframe = pd.DataFrame(rows)\n",
    "    public_bike_dataframe.sort_values(by=\"shared\", inplace=True) # inplace=True로 반영해야함\n",
    "    print(public_bike_dataframe)\n",
    "else:\n",
    "    print(\"error!!\")"
   ],
   "id": "6000680ea896d668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "if public_bike_dataframe is not None:\n",
    "    # Dataframe을 Json으로 변환\n",
    "    # force_ascii=False : 한글을 유니코드로 변환하지 않고 그대로 출력하도록 설정\n",
    "    # lines=False : JSON 데이터를 하나의 배열로 묶어서 저장\n",
    "    json_data = public_bike_dataframe.to_json(orient=\"records\", force_ascii=False, lines=False)\n",
    "\n",
    "    # JSON을 pretty-print로 변환해서 저장\n",
    "    parsed_json = json.loads(json_data)\n",
    "    with open(\"./data/public_bike_rent_information.json\", \"w\") as f:\n",
    "        json.dump(parsed_json, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "else:\n",
    "    print(\"error!!\")\n"
   ],
   "id": "c04ac21ae7edacde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. 도서관 정보나루 인기도서 데이터 수집 (API)\n",
    "import requests\n",
    "import dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "dotenv.load_dotenv(\".env\")\n",
    "API_KEY = os.getenv(\"PUBLIC_LIBRARY_API_KEY\")\n",
    "response = requests.get(\n",
    "    url=f\"https://data4library.kr/api/loanItemSrch?authKey={API_KEY}&&age=20&startDt=2025-01-01&endDt=2025-01-14&format=json\"\n",
    ")\n",
    "\n",
    "books = list()\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    for book in data[\"response\"][\"docs\"]:\n",
    "        books.append(book[\"doc\"])\n",
    "else:\n",
    "    print(\"error!!\")\n",
    "\n",
    "books_df = pd.DataFrame(books)\n",
    "books_df\n",
    "\n"
   ],
   "id": "a37f88a599377583",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "json_data = books_df.to_json(orient=\"records\", force_ascii=False, lines=False)\n",
    "\n",
    "# JSON을 pretty-print로 변환해서 저장\n",
    "parsed_json = json.loads(json_data)\n",
    "with open(\"./data/20s_best_book.json\", \"w\") as f:\n",
    "    json.dump(parsed_json, f, indent=4, ensure_ascii=False)\n"
   ],
   "id": "6711b029809c6a7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 웹 스크래핑으로 데이터 수집하기 - YES24 크롤링\n",
   "id": "b5feceff8c762c69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "books_df = pd.read_json('./data/20s_best_book.json')\n",
    "\n",
    "# 원하는 헤더만 선택\n",
    "# select_headers = [\"no\", \"ranking\", \"bookname\", \"authors\", \"publisher\", \"publication_year\", \"isbn13\"]\n",
    "# books_df = books_df[select_headers]\n",
    "\n",
    "# 또는 슬라이싱으로 선택\n",
    "books_df = books_df.loc[:, \"no\":\"isbn13\"]\n",
    "books_df.head()"
   ],
   "id": "56c4bcd82e8930c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 행과 열 선택해서 출력\n",
    "# 0행, 3행의 bookname열과 isbn열 출력\n",
    "books_df.loc[[0, 3], [\"bookname\", \"isbn13\"]]"
   ],
   "id": "e446bc8c5ab86596",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "async def fetch(session, url):\n",
    "    async with session.get(url) as resp:\n",
    "        return await resp.text()\n",
    "\n",
    "async def fetch_all(url_list):\n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        tasks = [fetch(session, url) for url in url_list]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "ISBN_list = books_df[\"isbn13\"].tolist()\n",
    "base_url = \"https://www.yes24.com\"\n",
    "book_url_list = [base_url + f\"/Product/Search?domain=BOOK&query={str(isbn)}\" for isbn in ISBN_list]\n",
    "\n",
    "async def main(url_list: list):\n",
    "    results = await fetch_all(url_list)\n",
    "    return results\n",
    "\n",
    "html_list = await main(book_url_list) # 빨간줄 나오는데, 무시해도 됨. Jupyter notebook은 이렇게 실행해야함"
   ],
   "id": "9073280ea9773b50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"{len(html_list)} 개 가져옴\")",
   "id": "f92a094143ba6bf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(html_list[0])",
   "id": "1949ad5bcfb7eb13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "book_detail_links = []\n",
    "for html in html_list:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    detail_link = soup.find('a', attrs={'class':'gd_name'}) # 개발자도구 켜고 select 버튼을 통해 상세 링크가 어디에 달려있는지 알 수 있음 -> class=gd_name의 a 태그에 존재함\n",
    "    if detail_link is None: # 가져온 html 구조에 이게 없을수도 있음 -> 무시\n",
    "        continue\n",
    "    book_detail_links.append(base_url + detail_link['href'])\n",
    "print(f\"{len(book_detail_links)} 개 가져옴\")\n",
    "print(f\"{book_detail_links[0]}\")"
   ],
   "id": "924281a6adc15160",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 상세페이지마다 들어가서 디테일 html 가져오기\n",
    "book_detail_htmls = await main(book_detail_links)\n",
    "print(book_detail_htmls[0])"
   ],
   "id": "e577c123b0143128",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 디테일 페이지에 있는 쪽수 정보 추출\n",
    "pages_list = []\n",
    "for html in book_detail_links:\n",
    "    soup = BeautifulSoup(book_detail_htmls[0], 'html.parser')\n",
    "    table = soup.find('table', attrs={'class':'tb_nor'})\n",
    "    tr_list = table.find_all('tr')\n",
    "    pages = tr_list[1].find('td').text.split('|')[0].strip()\n",
    "    pages_list.append(pages)\n",
    "\n",
    "print(len(pages_list))"
   ],
   "id": "158effcb29a044",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 전체 과정을 한번에 수행해보자",
   "id": "2cd043225a9753a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "books_df = pd.read_json('./data/20s_best_book.json').loc[:, \"no\":\"isbn13\"]\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "base_url = \"https://www.yes24.com\"\n",
    "ISBN_list = books_df[\"isbn13\"].tolist()\n",
    "book_url_list = [\n",
    "    f\"{base_url}/Product/Search?domain=BOOK&query={isbn}\" for isbn in ISBN_list\n",
    "]\n",
    "\n",
    "\n",
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url, timeout=10) as resp:\n",
    "            resp.raise_for_status()\n",
    "            return await resp.text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def fetch_all(url_list):\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        for url in url_list:\n",
    "            if url == 'NULL':\n",
    "                tasks.append(asyncio.sleep(0)) # HTML 파싱했을 때 링크가 없는 경우, 그냥 None 처리\n",
    "            else:\n",
    "                tasks.append(fetch(session, url))\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "async def fetch_detail_link(html_list):\n",
    "    result = []\n",
    "    for html in html_list:\n",
    "        if html is None:\n",
    "            result.append('NULL')\n",
    "            continue\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        link = soup.find('a', attrs={'class': 'gd_name'})\n",
    "        if link is None:\n",
    "            result.append('NULL')\n",
    "        else:\n",
    "            href = link.get('href', '')\n",
    "            full_link = base_url + href if href else 'NULL'\n",
    "            result.append(full_link)\n",
    "    return result\n",
    "\n",
    "async def parse_pages_info(html_list):\n",
    "    pages_list = []\n",
    "    for html in html_list:\n",
    "        if html is None:\n",
    "            pages_list.append('NULL')\n",
    "            continue\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        table = soup.find('table', attrs={'class': 'tb_nor'})\n",
    "        if table:\n",
    "            tr_list = table.find_all('tr')\n",
    "            if len(tr_list) > 1:\n",
    "                td = tr_list[1].find('td') # 쪽수 정보는 2번째 (배열기준 1번째)에 위치해 있음\n",
    "                if td:\n",
    "                    pages_text = td.get_text()\n",
    "                    pages = pages_text.split('|')[0].strip()\n",
    "                    pages_list.append(pages)\n",
    "                    continue\n",
    "        pages_list.append('NULL')  # 페이지 정보가 없을 경우\n",
    "    return pages_list\n",
    "\n",
    "async def main(url_list):\n",
    "    print(\"검색 페이지 HTML 리스트 가져오기...\")\n",
    "    search_htmls = await fetch_all(url_list)\n",
    "\n",
    "    print(\"검색 페이지 HTML에서 상세 링크 추출하기...\")\n",
    "    links = await fetch_detail_link(search_htmls)\n",
    "\n",
    "    print(\"각 상세링크마다 들어가서 HTML 정보 가져오기...\")\n",
    "    detail_htmls = await fetch_all(links)\n",
    "\n",
    "    print(\"각 상세페이지 HTML에서 Pages 정보 가져오기...\")\n",
    "    pages = await parse_pages_info(detail_htmls)\n",
    "\n",
    "    # 원본 DataFrame에 쪽수 정보 추가\n",
    "    pattern = re.compile(r'(\\d+)쪽$')\n",
    "    processed_pages = []\n",
    "    for page in pages:\n",
    "        if page == 'NULL':\n",
    "            processed_pages.append('NULL')\n",
    "            continue\n",
    "        match = pattern.search(page)\n",
    "        if match:\n",
    "            processed_pages.append(match.group(1))\n",
    "        else:\n",
    "            processed_pages.append('NULL')\n",
    "\n",
    "    books_df[\"pages\"] = processed_pages\n",
    "\n",
    "# 이벤트 루프를 사용하여 main 함수 실행\n",
    "if __name__ == \"__main__\":\n",
    "    await main(book_url_list)\n",
    "    print(\"데이터 처리 완료\")\n"
   ],
   "id": "2fe655f8645ee38c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "books_df.head(5)",
   "id": "1c1d2def8da91cfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 다음과 같은 DataFame df가 있을 때, loc 메서드의 결과가 다른 것은?\n",
    "\n",
    "|     | col1 | col2 |\n",
    "|-----|------|------|\n",
    "| 0   | a    | 1    |\n",
    "| 1   | b    | 2    |\n",
    "| 2   | c    | 3    |\n",
    "\n",
    "1. df.loc[[0,1,2], ['col1', 'col2']\n",
    "2. df.loc[0:2, 'col1': 'col2']\n",
    "3. df.loc[:2, 'col1': 'col2']\n",
    "4. df.loc[::2, 'col1': 'col2'] -> 이건 2칸씩 step이 증가하기 때문에 1,2,3과는 다른 결과가 나온다"
   ],
   "id": "ea1d2fbf50ac1690"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
